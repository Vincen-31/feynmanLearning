{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/COMP5625M/DeepLearningModule2023/blob/main/Classifier_week2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JnrmW256j5h"
   },
   "source": [
    "# Classifier using PyTorch\n",
    "\n",
    "In this notebook we train an MLP classifier on the MINST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wsRDuS836j5m",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\DevTools\\Anaconda\\envs\\geng\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'D:\\DevTools\\Anaconda\\envs\\geng\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BL0qsZ9h-MXb",
    "outputId": "5937a8eb-1a60-4155-a604-cc302e73edd5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check if gpu/cpu\n",
    "!python\n",
    "print(torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNKRIMHa6j5n",
    "tags": []
   },
   "source": [
    "Load the MNIST dataset. This will download a copy to your machine on first use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yul7987zUclB"
   },
   "source": [
    "# Mount your google drive to save your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rNSelVRnUikA",
    "outputId": "027b9d00-2346-452d-d07d-f456843947d1"
   },
   "outputs": [],
   "source": [
    "# Uncomment if you are using colab \n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # mounted on MyDrive\n",
    "# !ls '/content/drive/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fp87CbKyavQE"
   },
   "outputs": [],
   "source": [
    "# Define your result path in the Mydrive --> avoids re-running the same network after session \n",
    "\n",
    "ROOT = './'\n",
    "# uncomment if you are using colab\n",
    "# ROOT = '/content/drive/MyDrive/' \n",
    "\n",
    "DataPath = ROOT + 'Week2-data/'\n",
    "ResultPath = ROOT + 'Week2-results/'\n",
    "\n",
    "# make these directories if not available \n",
    "# --> this will avoid you from downloading the data again and again\n",
    "os.makedirs(DataPath, exist_ok=True)\n",
    "os.makedirs(ResultPath, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ic4L2wxW6j5n"
   },
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_set = torchvision.datasets.MNIST(\n",
    "    root=DataPath,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(\n",
    "    root=DataPath,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvHMBuKg6j5o"
   },
   "source": [
    "Inspect some of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gvlq1rM06j5o",
    "outputId": "4b7c6b86-e2bd-4864-fb60-cb95ea028830"
   },
   "outputs": [],
   "source": [
    "data, label = train_set[4]\n",
    "print(data.size())\n",
    "print(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6mBwyv26j5o"
   },
   "source": [
    "Create some iterable Data Loaders for easy iteration on mini-batches during training and testing. Also, initialise an array with the 10 class IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ljtjYWSY6j5o",
    "outputId": "ba6840f6-d1b8-474a-f0de-b3b948016120"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=24, # Forward pass only so batch size can be larger\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "classes = np.arange(0, 10)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NS_G1z9_6j5p"
   },
   "source": [
    "Show some images and labels as a sanity check.\n",
    "Use `torchvision.utils.make_grid` to create one image from a set of images. Note that this function converts single channel (grey-scale) tensors to have three channels. This is done by replicating the values into red, green and blue channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "id": "XvCXCVkg6j5p",
    "outputId": "11a2b1e6-b02b-41c2-b508-5fd9da188388"
   },
   "outputs": [],
   "source": [
    "def timshow(x):\n",
    "    xa = np.transpose(x.numpy(),(1,2,0))\n",
    "    plt.imshow(xa)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    return xa\n",
    "    \n",
    "# get a batch of random training examples (images and corresponding labels)\n",
    "dataiter = iter(train_loader)\n",
    "# images, labels = dataiter.next()\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images and labels\n",
    "print(images.size())\n",
    "timshow(torchvision.utils.make_grid(images))\n",
    "print(*labels.numpy())     # asterisk unpacks the ndarray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOa9FFk5UlRj"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hio-JytV6j5p"
   },
   "source": [
    "## Build a classifier\n",
    "Instead of defining the classifier function, loss function and parameter updates directly as we did in PyTorch.ipynb, it is convenient to use the `torch.nn` and `torch.optim` packages. These provide a simple way to build networks without losing sight of the iterative steps in gradient descent.\n",
    "\n",
    "First we construct the classifer function using the nn.Sequential wrapper that simply sequences the steps in the classifier function. In the case of a linear classifier there is just one nn.Linear layer. This is preceeded by `nn.Flatten` that vectorises a $28\\times28$ input image into a 1D vector of length $28*28$. We will also experiment with a two layer classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pwPcT3bW6j5q",
    "outputId": "6fcd379c-29ca-4173-8328-e129b59bf5e4"
   },
   "outputs": [],
   "source": [
    "net1 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    \n",
    "    # single layer\n",
    "    nn.Linear(28*28, 10)\n",
    "    \n",
    "\n",
    ")\n",
    "\n",
    "for param in net1.parameters():\n",
    "    print(param.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Tixg5i27dUD",
    "outputId": "654919ee-aa70-4ef9-8d8c-434e39079128"
   },
   "outputs": [],
   "source": [
    "net2 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "\n",
    "    # single layer\n",
    "    #nn.Linear(28*28, 10)\n",
    "    \n",
    "    # two layers\n",
    "    nn.Linear(28*28, 300),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(300,10)\n",
    ")\n",
    "\n",
    "for param in net2.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wphxAdlM6j5q"
   },
   "source": [
    "Train the network. For the two-layer network you'll need at least 200 epochs. 50 epochs will be more than enough for the one-layer network, but we'll run for the same number of epochs as for the two-layer network to give the full curve in the plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e-T55C3L6j5q",
    "outputId": "14987132-edd6-4b99-a427-a1ef29139d4b"
   },
   "outputs": [],
   "source": [
    "nepochs = 100    # number of epochs --> you can change as you want, for e.g., try 200 epochs\n",
    "net = net1       # 1-layer model\n",
    "results_path = ResultPath + 'linear1layer200epochs.pt'\n",
    "\n",
    "# initialise ndarray to store the mean loss in each epoch (on the training data)\n",
    "losses = np.zeros(nepochs)\n",
    "\n",
    "# Use a loss function and optimiser provided as part of PyTorch.\n",
    "# The chosen optimiser (Stochastic Gradient Descent with momentum) needs only to be given the parameters (weights and biases)\n",
    "# of the network and updates these when asked to perform an optimisation step below.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(nepochs):  # loop over the dataset multiple times\n",
    "\n",
    "    # initialise variables for mean loss calculation\n",
    "    running_loss = 0.0\n",
    "    n = 0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Zero the parameter gradients to remove accumulated gradient from a previous iteration.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward, backward, and update parameters\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # accumulate loss and increment minibatches\n",
    "        running_loss += loss.item()\n",
    "        n += 1\n",
    "       \n",
    "    # record the mean loss for this epoch and show progress\n",
    "    losses[epoch] = running_loss / n\n",
    "    print(f\"epoch: {epoch+1} loss: {losses[epoch] : .3f}\")\n",
    "    \n",
    "# save network parameters and losses\n",
    "torch.save({\"state_dict\": net.state_dict(), \"losses\": losses}, results_path)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1KZrbCB_38i",
    "outputId": "c21af3ab-d1b9-42e3-d130-3adac1e3610c"
   },
   "outputs": [],
   "source": [
    "nepochs = 100   # number of epochs --> you can change as you want\n",
    "net = net2       # 2-layer model\n",
    "results_path = ResultPath + '/linear2layer200epochs.pt'\n",
    "\n",
    "# initialise ndarray to store the mean loss in each epoch (on the training data)\n",
    "losses = np.zeros(nepochs)\n",
    "\n",
    "# Use a loss function and optimiser provided as part of PyTorch.\n",
    "# The chosen optimiser (Stochastic Gradient Descent with momentum) needs only to be given the parameters (weights and biases)\n",
    "# of the network and updates these when asked to perform an optimisation step below.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(nepochs):  # loop over the dataset multiple times\n",
    "\n",
    "    # initialise variables for mean loss calculation\n",
    "    running_loss = 0.0\n",
    "    n = 0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Zero the parameter gradients to remove accumulated gradient from a previous iteration.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward, backward, and update parameters\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # accumulate loss and increment minibatches\n",
    "        running_loss += loss.item()\n",
    "        n += 1\n",
    "       \n",
    "    # record the mean loss for this epoch and show progress\n",
    "    losses[epoch] = running_loss / n\n",
    "    print(f\"epoch: {epoch+1} loss: {losses[epoch] : .3f}\")\n",
    "    \n",
    "# save network parameters and losses\n",
    "torch.save({\"state_dict\": net.state_dict(), \"losses\": losses}, results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_21HMUZ06j5r"
   },
   "source": [
    "Notice that the first dimension of inputs and outputs corresponds to a minibatch of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "luzUWmSf6j5r",
    "outputId": "a2b65539-83b2-4d81-da8e-cbf0c83478a0"
   },
   "outputs": [],
   "source": [
    "print(f\"input size: {inputs.size()}, output size: {outputs.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NApOqVHH6j5r"
   },
   "source": [
    "Compare the history of the loss function during training (mean loss in each epoch) for 1 and 2 layer models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "CpUL3ZOG6j5s",
    "outputId": "b5f39f40-e0f2-4be7-f821-d6eb1d4605f4"
   },
   "outputs": [],
   "source": [
    "d1 = torch.load(ResultPath +  'linear1layer200epochs.pt')\n",
    "d2 = torch.load(ResultPath + 'linear2layer200epochs.pt')\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(d1[\"losses\"], 'r', label = '1 layer', )\n",
    "plt.plot(d2[\"losses\"], 'g', label = '2 layers' )\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training loss for 1 and 2 layer classifiers')\n",
    "\n",
    "fig.savefig(ResultPath + \"training_loss_MNIST.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2mx7xaB6j5s"
   },
   "source": [
    "How does the trained classifier `net` perform on the test set? First define our performance measures in terms of a given confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1yTxDyOq6j5s"
   },
   "outputs": [],
   "source": [
    "def accuracy(cnfm):\n",
    "    return cnfm.trace()/cnfm.sum((0,1))\n",
    "\n",
    "def recalls(cnfm):\n",
    "    return np.diag(cnfm)/cnfm.sum(1)\n",
    "\n",
    "def precisions(cnfm):\n",
    "    return np.diag(cnfm)/cnfm.sum(0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohYeBU736j5s"
   },
   "source": [
    "Run the model on test data, build a confusion matrix and compute performance measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "id": "l_DFIZOZ6j5s",
    "outputId": "1193838a-304c-4224-816c-723ab8296a3a"
   },
   "outputs": [],
   "source": [
    "d = torch.load(ResultPath + 'linear2layer200epochs.pt')\n",
    "# now load the parameter state into the current model (make sure this is the right model).\n",
    "net.load_state_dict(d[\"state_dict\"])\n",
    "\n",
    "# initialise confusion matrix\n",
    "nclasses = classes.shape[0]\n",
    "cnfm = np.zeros((nclasses,nclasses),dtype=int)\n",
    "\n",
    "# work without gradient computation since we are testing (i.e. no optimisation)\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        \n",
    "        # find the class with the highest output.\n",
    "        # note that the outputs are confidence values since we didn't need to apply softmax in our network\n",
    "        # (nn.crossentropyloss takes raw condifence values and does its own softmax)   \n",
    "        _, predicted = torch.max(outputs, 1)    \n",
    "       \n",
    "        \n",
    "        # accumulate into confusion matrix\n",
    "        for i in range(labels.size(0)):\n",
    "            cnfm[labels[i].item(),predicted[i].item()] += 1\n",
    "              \n",
    "print(\"Confusion matrix\")\n",
    "print(cnfm)\n",
    "\n",
    "# show confusion matrix as a grey-level image\n",
    "plt.imshow(cnfm, cmap='gray')\n",
    "\n",
    "# show per-class recall and precision\n",
    "print(f\"Accuracy: {accuracy(cnfm) :.1%}\")\n",
    "r = recalls(cnfm)\n",
    "p = precisions(cnfm)\n",
    "for i in range(nclasses):\n",
    "    print(f\"Class {classes[i]} : Precision {p[i] :.1%}  Recall {r[i] :.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkXbNvV-6j5t"
   },
   "source": [
    "## Defining a bespoke model class\n",
    "\n",
    "In the above, we have used the 'container' module `nn.Sequential` to define our network. To give more flexibility in the definition of the network, we can replace this with our own `nn.module` as below. Notice here, we have used this flexibility to perform the flattening ourselves instead of using `nn.Flatten` - this will be more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wwiQvmiM6j5t",
    "outputId": "bb470369-64d8-4a5d-b627-8d10b09968ec"
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = x.reshape(x.size(0), -1) # flatten the input\n",
    "        out = self.fc1(out)\n",
    "        return out\n",
    "    \n",
    "net = Classifier()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeMEU-3l6j5t"
   },
   "source": [
    "## Exercise\n",
    "Experiment with the impact on accuracy of (1) adding a third fully-connected linear layer to the network and (2) replacing `nn.Sigmoid` by `nn.ReLU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1be291f3dbd07eaa6ebc516f60b47b7db3c8d44b62eb2836d76961dfd5ef6a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
